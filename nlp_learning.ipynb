{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2de88d",
   "metadata": {},
   "source": [
    "#  Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45872841",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking a stream of text into smaller units called tokens. Tokens can be words, phrases, or symbols. Tokenization is a fundamental step in natural language processing (NLP) tasks, such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "There are many different ways to tokenize text. Some common methods include:\n",
    "\n",
    "* **Word tokenization:** This is the most common type of tokenization. It breaks text into individual words.\n",
    "* **Sentence tokenization:** This breaks text into individual sentences.\n",
    "* **Character tokenization:** This breaks text into individual characters.\n",
    "* **N-gram tokenization:** This breaks text into groups of N words. For example, 2-gram tokenization would break the sentence \"I love you\" into the tokens \"I\", \"love\", and \"you\".\n",
    "\n",
    "The type of tokenization that is used depends on the specific NLP task. For example, word tokenization is typically used for text classification tasks, while sentence tokenization is typically used for machine translation tasks.\n",
    "\n",
    "Tokenization is an important step in NLP because it allows computers to understand the structure of text. By breaking text into smaller units, tokenization makes it easier for computers to identify the meaning of words and phrases. This is essential for tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "Here are some examples of tokenization:\n",
    "\n",
    "* The sentence \"I love you\" can be tokenized as the words \"I\", \"love\", and \"you\".\n",
    "* The phrase \"the quick brown fox\" can be tokenized as the words \"the\", \"quick\", \"brown\", and \"fox\".\n",
    "* The acronym \"NLP\" can be tokenized as the words \"N\", \"L\", and \"P\".\n",
    "\n",
    "Tokenization is a powerful tool that can be used to improve the performance of NLP tasks. By breaking text into smaller units, tokenization makes it easier for computers to understand the structure of text and the meaning of words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e100e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7083e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization\n",
      "is\n",
      "a\n",
      "powerful\n",
      "tool\n",
      "that\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "text  = \"Tokenization is a powerful tool that can be used to improve the performance of NLP tasks. By breaking text into smaller units, tokenization makes it easier for computers to understand the structure of text and the meaning of words and phrases.\"\n",
    "doc = nlp(text)\n",
    "for token in doc[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea85017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token1=doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d49f7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3507d3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization | NOUN | noun\n",
      "is | AUX | auxiliary\n",
      "a | DET | determiner\n",
      "powerful | ADJ | adjective\n",
      "tool | NOUN | noun\n",
      "that | PRON | pronoun\n",
      "can | AUX | auxiliary\n",
      "be | AUX | auxiliary\n",
      "used | VERB | verb\n",
      "to | PART | particle\n"
     ]
    }
   ],
   "source": [
    "text  = \"Tokenization is a powerful tool that can be used to improve the performance of NLP tasks. By breaking text into smaller units, tokenization makes it easier for computers to understand the structure of text and the meaning of words and phrases.\"\n",
    "doc = nlp(text)\n",
    "for token in doc[:10]:\n",
    "    print(token ,\"|\",token.pos_,'|',spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8aa0d2",
   "metadata": {},
   "source": [
    "# Stemming \n",
    "\n",
    "Stemming is a process in natural language processing (NLP) that reduces inflected words to their word stem, base or root form. The stem is the part of the word that conveys the most meaning. For example, the words \"running\", \"ran\", and \"runs\" all have the same stem, which is \"run\". Stemming is often used in tasks such as text classification, information retrieval, and machine translation.\n",
    "\n",
    "There are two main types of stemming algorithms:\n",
    "\n",
    "* **Rule-based stemmers:** These algorithms use a set of rules to remove inflectional endings from words. For example, a rule-based stemmer might have a rule that removes the \"-ing\" ending from verbs.\n",
    "* **Statistical stemmers:** These algorithms use statistical methods to find the most likely stem for a word. For example, a statistical stemmer might look at the frequency of different word endings in a corpus of text to determine the most likely stem for a word.\n",
    "\n",
    "Stemming is a useful technique for NLP, but it is important to note that it is not perfect. Some stemming algorithms can produce incorrect results, and it is important to evaluate the performance of a stemming algorithm on a specific task before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c65620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meeting\n",
      "better | well\n"
     ]
    }
   ],
   "source": [
    "text=\"eating eats eat ate adjustable rafting ability meeting better\"\n",
    "doc=nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token ,'|',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46604039",
   "metadata": {},
   "source": [
    "# POS (Part of Speech)\n",
    "\n",
    "Part-of-speech (POS) tagging is a **natural language processing** (NLP) task of **categorizing each word in a text according to its part of speech**. Parts of speech (POS) are the **lexical categories** that **describe the syntactic role of a word** in a sentence.\n",
    "\n",
    "POS tagging is a **fundamental** NLP task, **useful for many other NLP tasks** such as **named entity recognition**, **parsing**, **and machine translation**.\n",
    "\n",
    "There are **two main approaches to POS tagging** :\n",
    "\n",
    "* **Rule-based** POS tagging uses **a set of hand-crafted rules** to assign POS tags to words.\n",
    "* **Statistical** POS tagging uses **a statistical model** to learn the probability of each word being assigned a particular POS tag.\n",
    "\n",
    "Statistical POS taggers are **more accurate** than rule-based POS taggers, but they **require a large corpus of text** to train the statistical model.\n",
    "\n",
    "**Here are some of the benefits of POS tagging:**\n",
    "\n",
    "* **It can help to disambiguate words** that have multiple meanings. For example, the word \"bank\" can be a noun or a verb. POS tagging can help to determine which meaning is intended in a particular context.\n",
    "* **It can help to identify the grammatical structure of a sentence**. For example, POS tagging can help to identify the subject, verb, and object of a sentence.\n",
    "* **It can help to identify the relationships between words in a sentence**. For example, POS tagging can help to identify the conjunctions that connect words and phrases.\n",
    "* **It can help to improve the accuracy of other NLP tasks** such as named entity recognition and machine translation.\n",
    "\n",
    "**Here are some of the challenges of POS tagging:**\n",
    "\n",
    "* **The accuracy of POS tagging can vary depending on the language**. Some languages are more ambiguous than others, which makes it more difficult to assign POS tags accurately.\n",
    "* **The accuracy of POS tagging can also vary depending on the corpus**. A corpus that is well-tagged will produce more accurate POS tags than a corpus that is not well-tagged.\n",
    "* **POS tagging can be computationally expensive**. Statistical POS taggers require a large corpus of text to train the statistical model, which can be time-consuming and expensive.\n",
    "\n",
    "**Despite the challenges, POS tagging is a valuable NLP task** that can be used to improve the accuracy of many other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef651cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I | PRP | pronoun, personal\n",
      "am | VBP | verb, non-3rd person singular present\n",
      "ate | VBN | verb, past participle\n",
      "apple | NN | noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "text1 = 'I am learning english'\n",
    "text2 = 'I am ate apple'\n",
    "# text3 = ''\n",
    "doc = nlp(text2)\n",
    "\n",
    "for token in doc:\n",
    "    print(token,'|',token.tag_,'|',spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35441c2f",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "Named Entity Recognition (NER) is a subtask of natural language processing (NLP) that identifies named entities in text and classifies them into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. NER is also used to extract structured information from unstructured text and is a key task in many NLP applications, such as question answering, information retrieval, and machine translation.\n",
    "\n",
    "Here are some examples of named entities:\n",
    "\n",
    "* Person names: John Smith, Jane Doe\n",
    "* Organization names: Google, Microsoft, Apple\n",
    "* Location names: New York City, London, Paris\n",
    "* Medical codes: ICD-10, CPT, HCPCS\n",
    "* Time expressions: 2023-04-29, 10:00 AM, 5 minutes\n",
    "* Quantities: 100, 5000, 1000000\n",
    "* Monetary values: $10, $50, $100\n",
    "* Percentages: 10%, 20%, 30%\n",
    "\n",
    "NER is a challenging task because it requires the ability to understand the context of a text and to identify the boundaries of named entities. There are a number of different approaches to NER, including rule-based, statistical, and machine learning-based approaches.\n",
    "\n",
    "Rule-based NER systems use a set of hand-crafted rules to identify named entities. These rules are typically based on the knowledge of a particular domain, such as medicine or finance. Statistical NER systems use statistical methods to identify named entities. These methods typically involve training a machine learning model on a large corpus of text that has been labeled with named entities. Machine learning-based NER systems use machine learning methods to identify named entities. These methods typically involve training a neural network on a large corpus of text that has been labeled with named entities.\n",
    "\n",
    "NER is a powerful tool that can be used to extract information from text. It is used in a variety of applications, such as:\n",
    "\n",
    "* Question answering: NER can be used to identify the entities that are mentioned in a question. This information can then be used to find the relevant information in a knowledge base.\n",
    "* Information retrieval: NER can be used to identify the entities that are mentioned in a document. This information can then be used to rank the document in a search results list.\n",
    "* Machine translation: NER can be used to identify the entities that are mentioned in a source text. This information can then be used to translate the entities into the target language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5b4188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
      "$45 billion | MONEY | Monetary values, including unit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_ , \"|\", spacy.explain(ent.label_))\n",
    "    \n",
    "    \n",
    "from spacy import displacy \n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2951a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
      "Twitter Inc | ORG | Companies, agencies, institutions, etc.\n",
      "$45 billion | MONEY | Monetary values, including unit\n",
      "Real Madrid | ORG | Companies, agencies, institutions, etc.\n",
      "Barcelona | GPE | Countries, cities, states\n",
      "Chelsea | GPE | Countries, cities, states\n",
      "Uzbekistan | GPE | Countries, cities, states\n",
      "Fergana | GPE | Countries, cities, states\n",
      "Oxford | ORG | Companies, agencies, institutions, etc.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Real Madrid\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " club \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barcelona\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chelsea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ",\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Uzbekistan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ",\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fergana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ",\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Oxford\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion Real Madrid club Barcelona, Chelsea,Uzbekistan,Fergana,Oxford\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_ , \"|\", spacy.explain(ent.label_))\n",
    "    \n",
    "print(displacy.render(doc,style='ent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0a170",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "A bag-of-words (BoW) model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.\n",
    "\n",
    "The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. An early reference to \"bag of words\" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure. The Bag-of-words model is one example of a Vector space model.\n",
    "\n",
    "To create a bag-of-words representation of a text, the following steps are typically performed:\n",
    "\n",
    "1. The text is tokenized, i.e., it is split into individual words or tokens.\n",
    "2. Stop words are removed. Stop words are common words that do not add much information to the representation, such as \"the\", \"a\", and \"of\".\n",
    "3. The words are stemmed or lemmatized. Stemming and lemmatization are processes that reduce words to their root form, i.e., the form of the word that is most similar to its meaning.\n",
    "4. The words are counted. The number of times each word appears in the text is recorded.\n",
    "5. The words are sorted by frequency. The words are sorted in descending order of frequency, i.e., the most frequent words are listed first.\n",
    "\n",
    "The bag-of-words representation of a text is a vector of word counts. The length of the vector is equal to the number of words in the vocabulary. The value of each element in the vector is the number of times the corresponding word appears in the text.\n",
    "\n",
    "The bag-of-words model is a simple and effective way to represent text. It is easy to understand and implement, and it can be used with a variety of machine learning algorithms. However, the bag-of-words model does not take into account the order of words in a text, which can be important for some tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6132fd",
   "metadata": {},
   "source": [
    "# N Gramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1087a30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 5, 'hatdowala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import   CountVectorizer\n",
    "\n",
    "v=CountVectorizer()\n",
    "\n",
    "v.fit([\"Thor Hatdowala is looking for job\"])\n",
    "v.vocabulary_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "777d31e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 9,\n",
       " 'hatdowala': 2,\n",
       " 'is': 4,\n",
       " 'looking': 7,\n",
       " 'for': 0,\n",
       " 'job': 6,\n",
       " 'thor hatdowala': 10,\n",
       " 'hatdowala is': 3,\n",
       " 'is looking': 5,\n",
       " 'looking for': 8,\n",
       " 'for job': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "v.fit([\"Thor Hatdowala is looking for job\"])\n",
    "v.vocabulary_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9fff520",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Thor ate pizza\",\n",
    "    \"Loki is tall\",\n",
    "    \"Loki is eating pizza\"\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    filtered_text=[]\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_text.append(token.lemma_)\n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62102237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loki eat pizza'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Loki is eating pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d7fcf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thor eat pizza', 'Loki tall', 'Loki eat pizza']\n"
     ]
    }
   ],
   "source": [
    "corpues_preprocessed = [preprocess(text) for text in corpus]\n",
    "print(corpues_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0afb15af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 7,\n",
       " 'eat': 0,\n",
       " 'pizza': 5,\n",
       " 'thor eat': 8,\n",
       " 'eat pizza': 1,\n",
       " 'loki': 2,\n",
       " 'tall': 6,\n",
       " 'loki tall': 4,\n",
       " 'loki eat': 3}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "v.fit(corpues_preprocessed)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ac4e2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Thor eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e806",
   "metadata": {},
   "source": [
    "# TF - IDF\n",
    "TF-IDF stands for term frequency-inverse document frequency. It is a statistical measure that is used to evaluate the importance of a word in a document within a collection or corpus. TF-IDF is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "The tf-idf is the product of two statistics, term frequency and inverse document frequency.\n",
    "\n",
    "* Term frequency (tf): This is the number of times a word appears in a document.\n",
    "* Inverse document frequency (idf): This is the logarithm of the number of documents in the corpus divided by the number of documents that contain the word.\n",
    "\n",
    "The tf-idf of a word in a document is calculated as follows:\n",
    "\n",
    "```\n",
    "tf-idf = tf * idf\n",
    "```\n",
    "\n",
    "TF-IDF is a powerful tool that can be used to improve the performance of a variety of natural language processing (NLP) tasks. For example, it can be used to:\n",
    "\n",
    "* Rank documents in a search results list\n",
    "* Identify important words in a document\n",
    "* Classify documents into different categories\n",
    "* Extract information from text\n",
    "\n",
    "TF-IDF is a simple but effective way to measure the importance of words in a document. It is a widely used technique in NLP and has been shown to be effective in a variety of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1bdeb5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\",\n",
    "    \"Apple is announcing new iphone tomorrow\",\n",
    "    \"Tesla is announcing new model-3 tomorrow\",\n",
    "    \"Google is announcing new pixel-6 tomorrow\",\n",
    "    \"Microsoft is announcing new surface tomorrow\",\n",
    "    \"Amazon is announcing new eco-dot tomorrow\",\n",
    "    \"I am eating biryani and you are eating grapes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "262bb7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thor': 25, 'eating': 10, 'pizza': 22, 'loki': 17, 'is': 16, 'ironman': 15, 'ate': 7, 'already': 0, 'apple': 5, 'announcing': 4, 'new': 20, 'iphone': 14, 'tomorrow': 26, 'tesla': 24, 'model': 19, 'google': 12, 'pixel': 21, 'microsoft': 18, 'surface': 23, 'amazon': 2, 'eco': 11, 'dot': 9, 'am': 1, 'biryani': 8, 'and': 3, 'you': 27, 'are': 6, 'grapes': 13}\n"
     ]
    }
   ],
   "source": [
    "v=TfidfVectorizer()\n",
    "transformed_output=v.fit_transform(corpus)\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ef014a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already 2.386294361119891\n",
      "am 2.386294361119891\n",
      "amazon 2.386294361119891\n",
      "and 2.386294361119891\n",
      "announcing 1.2876820724517808\n",
      "apple 2.386294361119891\n",
      "are 2.386294361119891\n",
      "ate 2.386294361119891\n",
      "biryani 2.386294361119891\n",
      "dot 2.386294361119891\n",
      "eating 1.9808292530117262\n",
      "eco 2.386294361119891\n",
      "google 2.386294361119891\n",
      "grapes 2.386294361119891\n",
      "iphone 2.386294361119891\n",
      "ironman 2.386294361119891\n",
      "is 1.1335313926245225\n",
      "loki 2.386294361119891\n",
      "microsoft 2.386294361119891\n",
      "model 2.386294361119891\n",
      "new 1.2876820724517808\n",
      "pixel 2.386294361119891\n",
      "pizza 2.386294361119891\n",
      "surface 2.386294361119891\n",
      "tesla 2.386294361119891\n",
      "thor 2.386294361119891\n",
      "tomorrow 1.2876820724517808\n",
      "you 2.386294361119891\n"
     ]
    }
   ],
   "source": [
    "all_feature_names = v.get_feature_names_out()\n",
    "\n",
    "for word in all_feature_names:\n",
    "    indx= v.vocabulary_.get(word)\n",
    "    print(f\"{word} {v.idf_[indx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72a83c",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "A word vector is a representation of a word as a vector of real numbers. Word vectors are typically learned from a large corpus of text, and they can be used to represent the meaning of words, to measure the similarity between words, and to perform other natural language processing tasks.\n",
    "\n",
    "There are a number of different ways to learn word vectors. One common approach is to use a neural network to predict the context of a word, given the word itself. The neural network learns to represent the meaning of words by predicting the words that are likely to appear around them.\n",
    "\n",
    "Another common approach to learning word vectors is to use a statistical method called latent semantic analysis (LSA). LSA uses a technique called singular value decomposition (SVD) to reduce the dimensionality of a matrix of word co-occurrences. The resulting vectors are then used to represent the meaning of words.\n",
    "\n",
    "Word vectors have been shown to be effective for a variety of natural language processing tasks, including:\n",
    "\n",
    "* **Text classification:** Word vectors can be used to represent the content of text documents, which can then be used to classify the documents into different categories.\n",
    "* **Named entity recognition:** Word vectors can be used to identify named entities in text, such as people, places, and organizations.\n",
    "* **Machine translation:** Word vectors can be used to translate text from one language to another.\n",
    "* **Question answering:** Word vectors can be used to answer questions about text documents.\n",
    "\n",
    "Word vectors are a powerful tool for natural language processing. They are able to represent the meaning of words in a way that is both informative and efficient. As a result, word vectors have been shown to be effective for a variety of natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882b133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d5487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8faa1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab48d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
